{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\r\n",
    "from spacy.lang.en import English\r\n",
    "import json\r\n",
    "import string\r\n",
    "import time\r\n",
    "import random\r\n",
    "from nltk.corpus import wordnet as wn\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import pandas as pd\r\n",
    "from Bio import Entrez\r\n",
    "from spacy.gold import GoldParse\r\n",
    "from spacy.scorer import Scorer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_data(file):\r\n",
    "    with open(file) as f:\r\n",
    "      data = json.load(f)\r\n",
    "    return(data)\r\n",
    "\r\n",
    "def save_data(file, data):\r\n",
    "    with open(file, 'w', encoding=\"utf-8\") as f:\r\n",
    "        json.dump(data, f, indent = 4)\r\n",
    "\r\n",
    "def process_data(data):\r\n",
    "    start = time.time()\r\n",
    "    my_list = []\r\n",
    "    for j in range(0,len(data)):\r\n",
    "        for i in range(0,len(data[j]['ingredients'])):\r\n",
    "            my_list.append(data[j]['ingredients'][i]['text'])\r\n",
    "\r\n",
    "    end = time.time()\r\n",
    "    print(\"Time: \", end-start)\r\n",
    "    return my_list\r\n",
    "\r\n",
    "def new_clean_proc(text):\r\n",
    "    start = time.time()\r\n",
    "    print(\"1\", text)\r\n",
    "    text = [x for x in text if not any(c.isdigit() for c in x)]\r\n",
    "    print(\"2\", text)\r\n",
    "    result = []\r\n",
    "    for i in range(0,len(text)):\r\n",
    "    \r\n",
    "        pro_text = [char.lower() for char in text[i] if char not in string.punctuation]\r\n",
    "        pro_text = ''.join(pro_text)\r\n",
    "        no_stop_pro_text = [word for word in pro_text.split() if word.lower() not in stopwords.words('english')]\r\n",
    "        no_stop_pro_text = ' '.join(no_stop_pro_text)\r\n",
    "        result.append(no_stop_pro_text)\r\n",
    "        print(\"3\", no_stop_pro_text)\r\n",
    "        \r\n",
    "\r\n",
    "    end = time.time()\r\n",
    "    print(\"Time: \", end-start)\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = load_data('det_ingrs.json')\r\n",
    "food = process_data(data)\r\n",
    "print(len(food))\r\n",
    "\r\n",
    "clean_food = new_clean_proc(food)\r\n",
    "print('Rozmiar: ', len(clean_food))\r\n",
    "print('Roznica: ', len(food) - len(clean_food))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def division(mylist, number):\r\n",
    "    five_list = []\r\n",
    "    one_word = []\r\n",
    "    for i in range(0,len(mylist)):\r\n",
    "        if ((len(mylist[i].split()) <= number) and (len(mylist[i].split()) > 1)):\r\n",
    "            five_list.append(mylist[i])\r\n",
    "        if len(mylist[i].split()) == 1:\r\n",
    "            one_word.append(mylist[i])\r\n",
    "    return five_list, one_word\r\n",
    "\r\n",
    "def separate(mylist):\r\n",
    "    result = []\r\n",
    "    for i in range(0,len(mylist)):\r\n",
    "        mylist[i] = mylist[i].split()\r\n",
    "        for j in range(0,len(mylist[i])):\r\n",
    "            result.append(mylist[i][j])\r\n",
    "    return result\r\n",
    "\r\n",
    "def if_none(list):\r\n",
    "    result = []\r\n",
    "    for i in range(0,len(list)):\r\n",
    "        if list[i]:\r\n",
    "            result.append(list[i])\r\n",
    "    return result\r\n",
    "\r\n",
    "    \r\n",
    "def if_food(word):\r\n",
    "    syns = wn.synsets(str(word), pos = wn.NOUN)\r\n",
    "    for syn in syns:\r\n",
    "        if 'food' in syn.lexname():\r\n",
    "            return word"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_food = list(set(clean_food))\r\n",
    "\r\n",
    "max_five_list, one_word = division(clean_food, 5)\r\n",
    "sep_food = separate(clean_food)\r\n",
    "print(\"Liczba podzielonych encji: \", len(sep_food))\r\n",
    "\r\n",
    "one_word_food = sep_food + one_word\r\n",
    "one_word_food = list(set(one_word_food))\r\n",
    "print(\"Liczba wszystkich jednowyrazowych encji: \", len(one_word_food))\r\n",
    "\r\n",
    "one_word_finally = if_none(list(map(if_food, one_word_food)))\r\n",
    "print(\"Koncowa liczba jednowyrazowych encji: \", len(one_word_finally))\r\n",
    "\r\n",
    "clean_food_after = one_word_finally + max_five_list\r\n",
    "print(\"Koncowy rozmiar danych: \", len(clean_food_after))\r\n",
    "\r\n",
    "etykiety = load_data(\"etykiety.json\")\r\n",
    "print(\"Liczba encji to usunięcia: \", len(etykiety))\r\n",
    "\r\n",
    "one_word_finally = list(filter(lambda x: x not in etykiety, one_word_finally))\r\n",
    "print(\"Po usunieciu: \", len(one_word_finally))\r\n",
    "\r\n",
    "clean_food_after = one_word_finally + max_five_list\r\n",
    "print(\"Koncowy rozmiar danych: \", len(clean_food_after))\r\n",
    "\r\n",
    "save_data('food_finally.json', clean_food_after)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def search(query, number):\r\n",
    "    Entrez.email = 'your.email@example.com'\r\n",
    "    handle = Entrez.esearch(db='pubmed',\r\n",
    "                            sort='relevance',\r\n",
    "                            retmax= number,\r\n",
    "                            retmode='xml',\r\n",
    "                            term=query)\r\n",
    "    results = Entrez.read(handle)\r\n",
    "    return results\r\n",
    "\r\n",
    "def fetch_details(id_list):\r\n",
    "    ids = ','.join(id_list)\r\n",
    "    Entrez.email = 'your.email@example.com'\r\n",
    "    handle = Entrez.efetch(db='pubmed',\r\n",
    "                           retmode='xml',\r\n",
    "                           id=ids)\r\n",
    "    results = Entrez.read(handle)\r\n",
    "    return results\r\n",
    "\r\n",
    "def get_abstract(words, number, filename):\r\n",
    "    start = time.time()\r\n",
    "    article = []\r\n",
    "    list_nr_id = []\r\n",
    "    \r\n",
    "    for word in words:\r\n",
    "        print(\"Słowo:\",word)\r\n",
    "        no_abstr = 0\r\n",
    "        proby = 0\r\n",
    "        yes_abstr = 0\r\n",
    "        results = search(word, number)\r\n",
    "        id_list = results['IdList']\r\n",
    "        papers = fetch_details(id_list)\r\n",
    "        for i, paper in enumerate(papers['PubmedArticle']):\r\n",
    "            proby+=1\r\n",
    "            try:\r\n",
    "                if paper['MedlineCitation']['Article']['Abstract']['AbstractText'] != None:\r\n",
    "                    article.append(paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0])\r\n",
    "                    list_nr_id.append(paper['MedlineCitation']['PMID'])\r\n",
    "                    yes_abstr += 1\r\n",
    "            except:\r\n",
    "                no_abstr += 1\r\n",
    "    \r\n",
    "        print(\"Podjęto prób ze słowem {}:\".format(word), proby)\r\n",
    "        print(\"Brak abstraktu ze słowem {}:\".format(word), no_abstr)\r\n",
    "        print(\"Ilość abstraktów ze słowem {}:\".format(word), yes_abstr)\r\n",
    "    print(\"Rozmiar zbioru abstaktów: \", len(article))\r\n",
    "    final = []\r\n",
    "    for i in range(len(article)):\r\n",
    "        final.append({'text': article[i], 'id': list_nr_id[i]})\r\n",
    "\r\n",
    "    save_data(filename, final)\r\n",
    "    \r\n",
    "\r\n",
    "    end = time.time()\r\n",
    "    hours, rem = divmod(end-start, 3600)\r\n",
    "    minutes, seconds = divmod(rem, 60)\r\n",
    "    print(\"Time:\", \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\r\n",
    "\r\n",
    "    return article\r\n",
    "\r\n",
    "\r\n",
    "def delete_duplicate(data):\r\n",
    "    seen = set()\r\n",
    "    result = []\r\n",
    "    for d in data:\r\n",
    "        t = tuple(d.items())\r\n",
    "        #print(t[0])\r\n",
    "        if t[0] not in seen:\r\n",
    "            seen.add(t[0])\r\n",
    "            result.append(d)\r\n",
    "\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_food = ['food', 'foods', 'vegetables', 'vegetable', \"fruit\", \"fruits\", \"grains\", \"meat\",\"meats\", \"nutriment\", \"nutriments\", \"dietary\", \"dietaries\", \"dairy\", \"drink\", \"drinks\", 'nutritious', 'dish', 'diet' , 'diets', 'cook', 'meal', 'meals', 'feed', 'nourishment', \"groceries\", \"grill\",  \"alimentary\", \"aliment\", \"pabulum\", \"cud\", 'allergy', 'allergies', 'spices', \"spice\", \"recipe\", \"recipes\"]\r\n",
    "abstr = get_abstract(list_food, 10000, 'last_list_abstract.json')\r\n",
    "\r\n",
    "abstr_list_data = load_data('last_list_abstract.json')\r\n",
    "res = delete_duplicate(abstr_list_data)\r\n",
    "save_data('last_abstract.json', res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def creating_data_training(data, type):\r\n",
    "    patterns = []\r\n",
    "    for item in data:\r\n",
    "        pattern = {\r\n",
    "                    \"label\": type,\r\n",
    "                    \"pattern\": item,\r\n",
    "        }\r\n",
    "        patterns.append(pattern)\r\n",
    "    return(patterns)\r\n",
    "\r\n",
    "def generate_rules(patterns):\r\n",
    "    nlp = English()\r\n",
    "    ruler = nlp.add_pipe(\"food_finally\")\r\n",
    "    ruler.add_patterns(patterns)\r\n",
    "    nlp.to_disk(\"patterns_food\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "patterns = creating_data_training(food, \"FOOD\")\r\n",
    "generate_rules(patterns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_model(model, text):\r\n",
    "    numb_ent = 0\r\n",
    "    nlp = model\r\n",
    "    doc = nlp(text)\r\n",
    "    results = []\r\n",
    "    entities = []\r\n",
    "    for ent in doc.ents:\r\n",
    "        entities.append((ent.start_char, ent.end_char, ent.label_))\r\n",
    "        numb_ent = numb_ent + 1\r\n",
    "    if len(entities) > 0:\r\n",
    "        results = [text, {'entities': entities}]\r\n",
    "    return (results, numb_ent)\r\n",
    "\r\n",
    "\r\n",
    "def create_train_data(nlp_model, name_save_data, article):\r\n",
    "    start = time.time()\r\n",
    "\r\n",
    "    fin_numb_ent = 0\r\n",
    "    TRAIN_DATA = []\r\n",
    "    nlp = spacy.load(nlp_model)\r\n",
    "\r\n",
    "    for i in range(len(article)):\r\n",
    "        try:\r\n",
    "                \r\n",
    "                (result, numb_ent) = test_model(nlp, article[i])\r\n",
    "                fin_numb_ent = numb_ent + fin_numb_ent\r\n",
    "\r\n",
    "                if result != []:\r\n",
    "                    TRAIN_DATA.append(result)\r\n",
    " \r\n",
    "        except:\r\n",
    "            continue\r\n",
    "    print('Wielkosc zbioru traningowego: ', len(TRAIN_DATA))\r\n",
    "    print('Liczba wykrytych encji: ', fin_numb_ent)\r\n",
    "    save_data(name_save_data, TRAIN_DATA)\r\n",
    "    end = time.time()\r\n",
    "    hours, rem = divmod(end-start, 3600)\r\n",
    "    minutes, seconds = divmod(rem, 60)\r\n",
    "    print(\"Time:\", \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "abstract_data = load_data(\"abstract_data.json\")\r\n",
    "TRAIN_DATA = create_train_data('patterns_food', 'test_food_finally.json ', abstract_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_food_spacy(TRAIN_DATA, iteration):\r\n",
    "    start = time.time()\r\n",
    "    nlp = spacy.blank(\"en\")\r\n",
    "    ner = nlp.create_pipe(\"ner\")\r\n",
    "    ner.add_label(\"FOOD\")\r\n",
    "    nlp.add_pipe(ner, name=\"test_food_ner\")\r\n",
    "    c = 0\r\n",
    "\r\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"test_food_ner\"]\r\n",
    "    with nlp.disable_pipes(*other_pipes):\r\n",
    "        optimizer = nlp.begin_training()\r\n",
    "        for itn in range(iteration):\r\n",
    "            start_part = time.time()\r\n",
    "            print(\"Start iteration: \", itn)\r\n",
    "            random.shuffle(TRAIN_DATA)\r\n",
    "            losses = {}\r\n",
    "            for text,annotations in TRAIN_DATA:\r\n",
    "                nlp.update(\r\n",
    "                            [text],\r\n",
    "                            [annotations],\r\n",
    "                            drop = 0.2,\r\n",
    "                            sgd = optimizer,\r\n",
    "                            losses = losses\r\n",
    "                )\r\n",
    "            end_part = time.time()\r\n",
    "            hours, rem = divmod(end_part-start_part, 3600)\r\n",
    "            minutes, seconds = divmod(rem, 60)\r\n",
    "            print(\"Time of iteration \",itn,\": \",\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\r\n",
    "    end = time.time()\r\n",
    "    hours, rem = divmod(end-start, 3600)\r\n",
    "    minutes, seconds = divmod(rem, 60)\r\n",
    "    print(\"Time:\", \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\r\n",
    "    return(nlp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random.shuffle(TRAIN_DATA)\r\n",
    "nlp = train_food_spacy(TRAIN_DATA, 30)\r\n",
    "nlp.to_disk(\"test_food_ner_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(nlp, examples):\r\n",
    "    ner_model = spacy.load(nlp)\r\n",
    "    scorer = Scorer()\r\n",
    "    for input_, annot in examples:\r\n",
    "        doc_gold_text = ner_model.make_doc(input_)\r\n",
    "        gold = GoldParse(doc_gold_text, entities=annot)\r\n",
    "        pred_value = ner_model(input_)\r\n",
    "        scorer.score(pred_value, gold)\r\n",
    "    \r\n",
    "    precision = scorer.scores['ents_p']\r\n",
    "    recall = scorer.scores['ents_r']\r\n",
    "    f1 = scorer.scores['ents_f']\r\n",
    "\r\n",
    "    print(\"Model: \", nlp)\r\n",
    "    print(scorer.scores)\r\n",
    "    print()\r\n",
    "    print(\"Precision: {:.2f}%\".format(precision))\r\n",
    "    print(\"Recall: {:.2f}%\".format(recall))\r\n",
    "    print(\"F1: {:.2f}%\".format(f1))\r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "def get_data(data):\r\n",
    "    examples = []\r\n",
    "    for i in range(len(data)):\r\n",
    "        spannn = []\r\n",
    "        for j in range(len(data[i]['spans'])):\r\n",
    "            spannn.append((data[i]['spans'][j]['start'], data[i]['spans'][j]['end'], data[i]['spans'][j]['label']))\r\n",
    "        examples.append((data[i]['text'], spannn))\r\n",
    "    return examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gold_standard = load_data('annoted_food.json')\r\n",
    "data = get_data(gold_standard)\r\n",
    "\r\n",
    "evaluate(\"food_ner_model\", data)\r\n",
    "evaluate(\"test_food_ner_model\", data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_food(df, nlp):\r\n",
    "    j=0\r\n",
    "    c=0\r\n",
    "    a=10\r\n",
    "    text = df['text']\r\n",
    "    df['food'] = 0\r\n",
    "    df['count_f'] = 0\r\n",
    "    for i in range(len(text)): \r\n",
    "        j+=1\r\n",
    "        if round(j*100 / len(text)) == a:\r\n",
    "            print(\"---------------- Progress: {:.0f}% ----------------\".format(a)) \r\n",
    "            a +=10\r\n",
    "        lists = []\r\n",
    "        doc = nlp(text[i])\r\n",
    "        for ent in doc.ents:\r\n",
    "            lists.append(ent.text)\r\n",
    "        if lists != []:\r\n",
    "            df['food'][i] = lists\r\n",
    "            df['count_f'][i] = len(lists)\r\n",
    "\r\n",
    "    return df\r\n",
    "\r\n",
    "nlp = spacy.load(\"test_food_ner_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = load_data(\"wyniki_z_bertowego_modelu.json\")\r\n",
    "df = pd.DataFrame(result, columns = ['text', 'id'])\r\n",
    "df_food = get_food(df, nlp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_med(df, nlp):\r\n",
    "    j=0\r\n",
    "    c=0\r\n",
    "    a=10\r\n",
    "    text = df['text']\r\n",
    "    df['med'] = 0\r\n",
    "    df['count_m'] = 0\r\n",
    "    for i in range(len(text)): \r\n",
    "        j+=1\r\n",
    "        if round(j*100 / len(text)) == a:\r\n",
    "            print(\"---------------- Progress: {:.0f}% ----------------\".format(a)) \r\n",
    "            a +=10\r\n",
    "        lists = []\r\n",
    "        doc = nlp(text[i])\r\n",
    "        for ent in doc.ents:\r\n",
    "            if ent.label_ == \"DISEASE\":\r\n",
    "                lists.append(ent.text)\r\n",
    "        if lists != []:\r\n",
    "            df['med'][i] = lists\r\n",
    "            df['count_m'][i] = len(lists)\r\n",
    "    return df\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_med = get_med(df_food, nlp)\r\n",
    "df_med.to_csv('bertowy_model_z_food_med.csv', index = False, header = True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}