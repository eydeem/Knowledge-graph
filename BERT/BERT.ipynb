{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "import transformers\r\n",
        "from transformers import AutoModel, BertTokenizerFast\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "import json\r\n",
        "\r\n",
        "device = torch.device(\"cuda\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "x4giRzM7NtHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_train = pd.read_csv(\"df_train.csv\")\r\n",
        "df_test = pd.read_csv(\"df_test.csv\")\r\n",
        "\r\n",
        "\r\n",
        "train_text = df_train['text']\r\n",
        "train_labels = df_train['answer']\r\n",
        "\r\n",
        "test_text = df_test['text']\r\n",
        "test_labels = df_test['answer']\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "val_text, train_text, val_labels, train_labels = train_test_split(train_text, train_labels, \r\n",
        "                                                                random_state=42, \r\n",
        "                                                                test_size=0.7, \r\n",
        "                                                                stratify=train_labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "1x9H-KxpAACt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert = AutoModel.from_pretrained('bert-base-uncased')\r\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "max_seq_len = 50\r\n",
        "\r\n",
        "tokens_train = tokenizer.batch_encode_plus(\r\n",
        "    train_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")\r\n",
        "\r\n",
        "tokens_val = tokenizer.batch_encode_plus(\r\n",
        "    val_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")\r\n",
        "\r\n",
        "tokens_test = tokenizer.batch_encode_plus(\r\n",
        "    test_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "tk5S7DWaP2t6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'])\r\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\r\n",
        "train_y = torch.tensor(train_labels.tolist())\r\n",
        "\r\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\r\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\r\n",
        "val_y = torch.tensor(val_labels.tolist())\r\n",
        "\r\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\r\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\r\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "outputs": [],
      "metadata": {
        "id": "QR-lXwmzQPd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\r\n",
        "val_sampler = SequentialSampler(val_data)\r\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "qUy9JKFYQYLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "for param in bert.parameters():\r\n",
        "    param.requires_grad = False"
      ],
      "outputs": [],
      "metadata": {
        "id": "wHZ0MC00RQA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class BERT_Arch(nn.Module):\r\n",
        "    def __init__(self, bert):\r\n",
        "      super(BERT_Arch, self).__init__()\r\n",
        "      self.bert = bert   \r\n",
        "      self.dropout = nn.Dropout(0.1)\r\n",
        "      self.relu =  nn.ReLU()\r\n",
        "      self.fc1 = nn.Linear(768,512)\r\n",
        "      self.fc2 = nn.Linear(512,2)\r\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, sent_id, mask): \r\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\r\n",
        "      x = self.fc1(cls_hs)\r\n",
        "      x = self.relu(x)\r\n",
        "      x = self.dropout(x)\r\n",
        "      x = self.fc2(x)\r\n",
        "      x = self.softmax(x)\r\n",
        "      return x\r\n",
        "\r\n",
        "model = BERT_Arch(bert)\r\n",
        "model = model.to(device)\r\n",
        "\r\n",
        "\r\n",
        "from transformers import AdamW\r\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "b3iEtGyYRd0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\r\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\r\n",
        "print(class_wts)\r\n",
        "\r\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\r\n",
        "weights = weights.to(device)\r\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "def train():\r\n",
        "  model.train()\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  total_preds=[]\r\n",
        "  for step,batch in enumerate(train_dataloader):\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n",
        "    batch = [r.to(device) for r in batch]\r\n",
        "    sent_id, mask, labels = batch\r\n",
        "    model.zero_grad()        \r\n",
        "    preds = model(sent_id, mask)\r\n",
        "    loss = cross_entropy(preds, labels)\r\n",
        "    total_loss = total_loss + loss.item()\r\n",
        "    loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "    optimizer.step()\r\n",
        "    preds=preds.detach().cpu().numpy()\r\n",
        "    total_preds.append(preds)\r\n",
        "  avg_loss = total_loss / len(train_dataloader)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "  return avg_loss, total_preds\r\n",
        "\r\n",
        "def evaluate():\r\n",
        "  model.eval()\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  total_preds = []\r\n",
        "  for step,batch in enumerate(val_dataloader):\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      elapsed = format_time(time.time() - t0) \r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n",
        "    batch = [t.to(device) for t in batch]\r\n",
        "    sent_id, mask, labels = batch\r\n",
        "    with torch.no_grad():\r\n",
        "      preds = model(sent_id, mask)\r\n",
        "      loss = cross_entropy(preds,labels)\r\n",
        "      total_loss = total_loss + loss.item()\r\n",
        "      preds = preds.detach().cpu().numpy()\r\n",
        "      \r\n",
        "      total_preds.append(preds)\r\n",
        "  avg_loss = total_loss / len(val_dataloader) \r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  return avg_loss, total_preds"
      ],
      "outputs": [],
      "metadata": {
        "id": "rskLk8R_SahS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_valid_loss = float('inf')\r\n",
        "train_losses=[]\r\n",
        "valid_losses=[]\r\n",
        "epochs = 30\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n",
        "    train_loss, _ = train()\r\n",
        "    valid_loss, _ = evaluate()\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights_new.pt')\r\n",
        "    train_losses.append(train_loss)\r\n",
        "    valid_losses.append(valid_loss)\r\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "x = range(1,len(train_losses)+1)\r\n",
        "\r\n",
        "plt.figure(figsize=(16, 5))\r\n",
        "plt.plot(x, train_losses, 'b', label='Training loss')\r\n",
        "plt.plot(x, valid_losses, 'r', label='Validation loss')\r\n",
        "plt.title('Training and validation loss')\r\n",
        "plt.grid()\r\n",
        "plt.legend()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "path = 'saved_weights_new.pt'\r\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ],
      "metadata": {
        "id": "OacxUyizS8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae8e29b-972a-4748-a89b-61c895d1759b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch\r\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\r\n",
        "\r\n",
        "saved_model = torch.load('bertowy_model')\r\n",
        "\r\n",
        "def evaluate_roc(probs, y_true):\r\n",
        "\r\n",
        "    preds = probs\r\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\r\n",
        "    roc_auc = auc(fpr, tpr)\r\n",
        "    print(f'AUC: {roc_auc:.4f}')\r\n",
        "       \r\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\r\n",
        "    accuracy = accuracy_score(y_true, y_pred)\r\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\r\n",
        "    \r\n",
        "    plt.title('Receiver Operating Characteristic')\r\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\r\n",
        "    plt.legend(loc = 'lower right')\r\n",
        "    plt.plot([0, 1], [0, 1],'r--')\r\n",
        "    plt.xlim([0, 1])\r\n",
        "    plt.ylim([0, 1])\r\n",
        "    plt.ylabel('True Positive Rate')\r\n",
        "    plt.xlabel('False Positive Rate')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  preds = saved_model(test_seq.to(device), test_mask.to(device))\r\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "giSb3tfpKNkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "preds = np.argmax(preds, axis = 1)\r\n",
        "print(classification_report(test_y, preds))\r\n",
        "\r\n",
        "evaluate_roc(preds, test_y)\r\n",
        "\r\n",
        "pd.crosstab(test_y, preds)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import time"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def split_sentence_id(data, id):\r\n",
        "  list_sentence = []\r\n",
        "  list_id = []\r\n",
        "  result = []\r\n",
        "  for i in id:\r\n",
        "      part_sentence = nltk.tokenize.sent_tokenize(data[i]['text']) \r\n",
        "      list_sentence.append(part_sentence)\r\n",
        "      for j in range(len(part_sentence)):\r\n",
        "        list_id.append(data[i]['id'])\r\n",
        "  sentence_text = [item for sublist in list_sentence for item in sublist]\r\n",
        "  for i in range(len(sentence_text)):\r\n",
        "      if sentence_text[i][-1] != '.':\r\n",
        "          sentence_text[i] = sentence_text[i] + '.'\r\n",
        "\r\n",
        "  for i in range(len(sentence_text)):\r\n",
        "    result.append({'text' : sentence_text[i], 'id': list_id[i]})\r\n",
        "\r\n",
        "\r\n",
        "  return result\r\n",
        "\r\n",
        "\r\n",
        "def load_data(file):\r\n",
        "    with open(file) as f:\r\n",
        "      data = json.load(f)\r\n",
        "    return(data)\r\n",
        "\r\n",
        "\r\n",
        "def save_data(file, data):\r\n",
        "    with open(file, 'w', encoding=\"utf-8\") as f:\r\n",
        "        json.dump(data, f, indent = 4)\r\n",
        "\r\n",
        "\r\n",
        "def filtr_model(data, model):\r\n",
        "    start_part = time.time()\r\n",
        "\r\n",
        "    i=0\r\n",
        "    c=0\r\n",
        "    a=10\r\n",
        "    result_abstract = []\r\n",
        "    zbior = data\r\n",
        "    \r\n",
        "    \r\n",
        "    for text in zbior:\r\n",
        "        i+=1\r\n",
        "        if round(i*100 / len(zbior)) == a:\r\n",
        "            print(\"---------------- Progress: {:.0f}% ----------------\".format(a)) \r\n",
        "            a +=10\r\n",
        "\r\n",
        "        vec = pd.DataFrame(vectd.transform([text]).todense())\r\n",
        "        result = model.predict(vec)\r\n",
        "        if result == [1]:\r\n",
        "            c += 1\r\n",
        "            result_abstract.append(text)\r\n",
        "    end_part = time.time()\r\n",
        "    hours, rem = divmod(end_part-start_part, 3600)\r\n",
        "    minutes, seconds = divmod(rem, 60)\r\n",
        "\r\n",
        "    print(\"Czas: \",\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\r\n",
        "    print('\\n')\r\n",
        "    print(\"Wielkośc zbioru: \", len(zbior))\r\n",
        "    print(\"Wynik:\", c)\r\n",
        "    print(\"Procent zaakceptowanych: {:.2f}%\".format(c*100/len(zbior)))\r\n",
        "\r\n",
        "    save_data('filtered_data.json', result_abstract)\r\n",
        "    return result_abstract"
      ],
      "outputs": [],
      "metadata": {
        "id": "jpX1uTwjUPY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "abstract = load_data('last_abstract.json')\r\n",
        "data = pd.Series(abstract)\r\n",
        "split_data = split_sentence_id(data)\r\n",
        "len(split_data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gxkrzTlExRS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch, gc\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "metadata": {
        "id": "e2i2bnTg2V-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with torch.no_grad():\r\n",
        "\r\n",
        "  preds = model(abs_seq.to(device), abs_mask.to(device))\r\n",
        "  preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "preds = np.argmax(preds, axis = 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "W_z999901w_6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_model = torch.load('bertowy_model')"
      ],
      "outputs": [],
      "metadata": {
        "id": "F9P7sQcG0WyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def predict_sentence(model, numb):\r\n",
        "  dummy = 0\r\n",
        "  for i in range(numb):\r\n",
        "    split_list = []\r\n",
        "    for j in range(200):\r\n",
        "      split_list.append(split_data[i*200:(i+1)*200][j]['text'])\r\n",
        "    split_data_part = pd.Series(split_list)\r\n",
        "\r\n",
        "    max_seq_len = 50\r\n",
        "    tokens_split = tokenizer.batch_encode_plus(\r\n",
        "        split_data_part.tolist(),\r\n",
        "        max_length = max_seq_len,\r\n",
        "        pad_to_max_length=True,\r\n",
        "        truncation=True,\r\n",
        "        return_token_type_ids=False\r\n",
        "    ) \r\n",
        "    abs_seq = torch.tensor(tokens_split['input_ids'])\r\n",
        "    abs_mask = torch.tensor(tokens_split['attention_mask'])\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "      preds = model(abs_seq.to(device), abs_mask.to(device))\r\n",
        "      preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "    preds = np.argmax(preds, axis = 1)\r\n",
        "    dummy = np.append(dummy, preds)\r\n",
        "    if(i%200) == 0:\r\n",
        "      print((i+1), \":\", dummy.shape)\r\n",
        "\r\n",
        "  return np.delete(dummy, 0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RDh-tCMc1w6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_acc_sentence(model, numb):\r\n",
        "    data = predict_sentence(model, numb)\r\n",
        "    a = 0\r\n",
        "    final_sentence = []\r\n",
        "    print(len(data))\r\n",
        "    for i in range(len(data)):\r\n",
        "      if data[i] == 1:\r\n",
        "        a += 1\r\n",
        "        final_sentence.append(split_data[i])\r\n",
        "\r\n",
        "    print(\"Liczba zdań: \", len(data))\r\n",
        "    print(\"Liczba zaakceptowanych: \", a)\r\n",
        "    print(\"Procent zaakceptowanych: {:.2f}%\".format(a*100/len(data)))\r\n",
        "    \r\n",
        "    return final_sentence"
      ],
      "outputs": [],
      "metadata": {
        "id": "uFbZ_Pqh-v8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "final_sent = get_acc_sentence(bert_model, 6000) \r\n",
        "save_data('wyniki_z_bertowego_modelu.json', final_sent)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}